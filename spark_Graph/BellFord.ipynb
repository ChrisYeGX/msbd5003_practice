{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 µs, sys: 0 ns, total: 20 µs\n",
      "Wall time: 23.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "def getVerticeList(data,flag):\n",
    "    \"\"\"This function is used to obtain the list which shows all the vertices in graph\"\"\"\n",
    "    file = open(data)\n",
    "    text = file.read().decode(\"utf-8\")\n",
    "    file.close()\n",
    "    text = text.split('\\n')\n",
    "    verticeList = []\n",
    "    outverticeList=[]\n",
    "    for i in text:\n",
    "        sentence = i.split(' ')\n",
    "        verticeList.append(str(sentence[0]))\n",
    "        outverticeList.append(str(sentence[0]))\n",
    "        verticeList.append(str(sentence[1]))\n",
    "    verticeList = list(verticeList)\n",
    "    outverticeList = list(set(outverticeList))\n",
    "    addvertice=[]\n",
    "    for i in verticeList:\n",
    "        if i not in outverticeList:\n",
    "            addvertice.append(i)\n",
    "    if flag == 'a':\n",
    "        return verticeList\n",
    "    else :\n",
    "        return addvertice\n",
    "\n",
    "def splitVE(line):\n",
    "    \"\"\"This function split a row of data into two parts: vertice and edges\n",
    "       The output is a tuple which contains a string of vertice name and a list of it's outward edges\n",
    "    \"\"\"\n",
    "   # line = str(cline).split(' ')\n",
    "    line = str(line).split(' ')\n",
    "    ver = line[0]\n",
    "    edges = [(line[1], float(line[2]))]\n",
    "    return (ver, edges)\n",
    "\n",
    "def fillINF(line, data):\n",
    "    \"\"\"This function will fill in the blanks in an edges list\n",
    "       That is fill all the target vertice with weight of 'infinity' into the list\n",
    "       If the target vertice is identical to the input vertice, fill in the weight with 0\n",
    "    \"\"\"\n",
    "    edges = line[1]\n",
    "    needToFill = getVerticeList(data,'a')[:]\n",
    "    print(needToFill)\n",
    "    for i in edges:\n",
    "        needToFill.remove(i[0])\n",
    "    for j in needToFill:\n",
    "        if line[0] == j:\n",
    "            line[1].append((j, 0))\n",
    "        else:\n",
    "            line[1].append((j, float(\"+inf\")))\n",
    "    outputEdges = line[1]\n",
    "    outputEdges = sorted(outputEdges, key=lambda x: x[0])\n",
    "    return (line[0],outputEdges)\n",
    "\n",
    "def fillLENGTH(vertice, start):\n",
    "    \"\"\"This function can be used to add the path weight from start to the current vertex\n",
    "       Since it's the first step, if start is the same with vertice, the path is 0\n",
    "       otherwise, fill in 'infinity'\n",
    "    \"\"\"\n",
    "    edge =vertice[1][:]\n",
    "    ver = vertice[0]\n",
    "    if start == ver:\n",
    "        return (ver, (0, edge))\n",
    "    else:\n",
    "        return (ver, (float(\"+inf\"), edge))\n",
    "\n",
    "def getOriginalMatrix(data, start):\n",
    "    \"\"\"This function returns a complete path adjacency matrix of the start node\n",
    "       Each line stores three parts:\n",
    "       The first is a vertex name means 'from start to this vertex'\n",
    "       Then the second part store the length from start to this vertex\n",
    "       The final part stores from this vertex to all other vertices with the corresponding weight\n",
    "       \"\"\"\n",
    "    #data2=\"result.txt\"\n",
    "    graph = sc.textFile(data)\n",
    "    formedGraph = graph.map(lambda x: splitVE(x))\n",
    "    addvertice= getVerticeList(data,'b')\n",
    "    newrdd=[]\n",
    "    for i in addvertice:\n",
    "        newrdd.append((i,[(i,0.0)]))\n",
    "    formedGraph.collect()  \n",
    "    newrdd=sc.parallelize(newrdd,4)\n",
    "    newrdd.cache()\n",
    "    formedGraph=formedGraph.union(newrdd)\n",
    "    filledGraph = formedGraph.reduceByKey(lambda x, y: x+y) \n",
    "    filledINFGraph = filledGraph.map(lambda x: fillINF(x, data))\n",
    "    filledLENGraph = filledINFGraph.map(lambda x: fillLENGTH(x, start))\n",
    "    filledLENGraph.cache()\n",
    "    return filledLENGraph\n",
    "\n",
    "def getAdjDict(data, start):\n",
    "    \"\"\"This function calculates the dictionary within values of outward edges and weights,\n",
    "       The key of this dictionary is the start\n",
    "    \"\"\"\n",
    "    adj = getOriginalMatrix(data, start).collect()\n",
    "    adjDict = dict()\n",
    "    for i in adj:\n",
    "        adjDict[i[0]] = i[1][1]\n",
    "    return adjDict\n",
    "\n",
    "def newPath(vertice, adjDict):\n",
    "    \"\"\"This function update the weights to the length from start to B via A\n",
    "       Where A and B can be all the vertices in graph\n",
    "    \"\"\"\n",
    "    known = vertice[1][0]\n",
    "    edges = vertice[1][1][:]\n",
    "    new = []\n",
    "    for i in edges:\n",
    "        newPath = i[1]+known\n",
    "        new.append((i[0], (newPath, adjDict[i[0]])))\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestPath(data, start):\n",
    "    \"\"\"This part is the core of Bell-Ford Shortest Path Algorithm, \n",
    "       from the start to the end, it compare all the known weights of path via other nodes\n",
    "       choosing the shortest one and update, and follow the loops again.\n",
    "       The loop will stop when there are no path be updated\n",
    "    \"\"\"\n",
    "    verticeList = getVerticeList(data,'a')\n",
    "    originalMatrix = getOriginalMatrix(data, start)\n",
    "    adjDictionary = getAdjDict(data, start)\n",
    "    flag = 0\n",
    "    \n",
    "    update = True\n",
    "    origin = getOriginalMatrix(data, start)\n",
    "    new = getOriginalMatrix(data, start)\n",
    "    adj = getAdjDict(data, start)\n",
    "    while update:\n",
    "        flag += 1\n",
    "        origin = new\n",
    "        newMAPGraph = new.flatMap(lambda x: newPath(x, adj))\n",
    "        #print(newMAPGraph.collect())\n",
    "        new = newMAPGraph.reduceByKey(lambda x, y: x if x[0]<=y[0] else y) \n",
    "        #print('ssssssssss')\n",
    "        #print(new.collect()) \n",
    "        origin.cache()\n",
    "        new.cache()\n",
    "        if new.collect() == origin.collect():\n",
    "            update = False\n",
    "        elif flag == len(adjDictionary)-1:\n",
    "            update = False\n",
    "    shortestList = new.map(lambda x: (x[0], x[1][0])).collect()\n",
    "    return sorted(shortestList, key = lambda x: x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AbGhurayb,Iraq', 622.5),\n",
       " ('Abadan,Iran', 0.0),\n",
       " ('Abha,SaudiArabia', 2393.1),\n",
       " ('AbuDhabi,UnitedArabEmirates', 1433.09),\n",
       " ('AdDnyah,Iraq', 444.8),\n",
       " ('Adana,Turkey', 1807.41),\n",
       " ('Aden,Yemen', 3266.91),\n",
       " ('Adyaman,Turkey', 1584.88)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortestPath('smalldemo.txt', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', inf), ('2', inf), ('3', inf)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortestPath('testGraph.txt', 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/testGraph.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-17a0819dfa24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshortestPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/testGraph.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bd5cb1473d3f>\u001b[0m in \u001b[0;36mshortestPath\u001b[0;34m(data, start)\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0mThe\u001b[0m \u001b[0mloop\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mstop\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mno\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverticeList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVerticeList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moriginalMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOriginalMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0madjDictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAdjDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mgetVerticeList\u001b[0;34m(data)\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/testGraph.txt'"
     ]
    }
   ],
   "source": [
    "shortestPath('data/testGraph.txt', '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('world.txt', 'w')\n",
    "for ip in final:\n",
    "        \"    fileObject.write(ip)\\n\",\n",
    "    \"    fileObject.write('\\\\n')\\n\",\n",
    "    \"fileObject.close()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u016b' in position 2: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-b1320ec66b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u016b' in position 2: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "file = open(\"input.txt\")\n",
    "text = file.read().decode(\"utf-8\")\n",
    "file.close()\n",
    "text = text.split('\\n')\n",
    "a=text[3].split(' ')\n",
    "str(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abadan,Iran Bah\\xc3\\xa7elievler,Turkey 1678.7', 1),\n",
       " ('Abadan,Iran Ansan-si,SouthKorea 9477.22', 1),\n",
       " ('Abadan,Iran AdD\\xc4\\xabw\\xc4\\x81n\\xc4\\xabyah,Iraq 444.8', 1),\n",
       " ('Abadan,Iran BandarLampung,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Abha,SaudiArabia 2393.1', 1),\n",
       " ('Abadan,Iran Ajmer,India 4139.16', 1),\n",
       " ('Abadan,Iran Akashi,Japan 10724.94', 1),\n",
       " ('Abadan,Iran Ankara,Turkey 1665.9', 1),\n",
       " ('Abadan,Iran Akola,India 4986.06', 1),\n",
       " ('Abadan,Iran AlA\\xe1\\xb8\\xa9mad\\xc4\\xab,Kuwait 269.25', 1),\n",
       " ('Abadan,Iran AlKharj,SaudiArabia 1141.93', 1),\n",
       " ('Abadan,Iran Al\\xe1\\xb8\\xa8illah,Iraq 532.44', 1),\n",
       " ('Abadan,Iran Baku,Azerbaijan 1996.23', 1),\n",
       " ('Abadan,Iran Andijon,Uzbekistan 3300.46', 1),\n",
       " ('Abadan,Iran Bangkok,Thailand 10870.33', 1),\n",
       " ('Abadan,Iran Almaty,Kazakhstan 3857.16', 1),\n",
       " ('Abadan,Iran Ahmadnagar,India 5113.66', 1),\n",
       " ('Abadan,Iran Aden,Yemen 3266.91', 1),\n",
       " ('Abadan,Iran Ardab\\xc4\\xabl,Iran 1635.73', 1),\n",
       " ('Abadan,Iran AlorSetar,Malaysia 11940.61', 1),\n",
       " ('Abadan,Iran Ageoshimo,Japan 11347.74', 1),\n",
       " ('Abadan,Iran Anyang,China 8199.56', 1),\n",
       " ('Abadan,Iran Adapazar\\xc4\\xb1,Turkey 1666.08', 1),\n",
       " ('Abadan,Iran Atsugi,Japan 11261.11', 1),\n",
       " ('Abadan,Iran Agra,India 4105.09', 1),\n",
       " ('Abadan,Iran Amarn\\xc4\\x81th,India 5108.96', 1),\n",
       " ('Abadan,Iran BacolodCity,Philippines 0.0', 1),\n",
       " ('Abadan,Iran AlJubayl,SaudiArabia 573.98', 1),\n",
       " ('Abadan,Iran Aurangabad,India 5035.58', 1),\n",
       " ('Abadan,Iran Ab\\xc5\\xabGhurayb,Iraq 622.5', 1),\n",
       " ('Abadan,Iran Banjarmasin,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Asahikawa,Japan 12408.7', 1),\n",
       " ('Abadan,Iran AlK\\xc5\\xabt,Iraq 568.14', 1),\n",
       " ('Abadan,Iran BandarAbbas,Iran 1124.57', 1),\n",
       " ('Abadan,Iran Antipolo,Philippines 0.0', 1),\n",
       " ('Abadan,Iran Anshan,China 8781.29', 1),\n",
       " ('Abadan,Iran Bacoor,Philippines 0.0', 1),\n",
       " ('Abadan,Iran Balikpapan,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Aksu,China 4483.61', 1),\n",
       " ('Abadan,Iran AlMubarraz,SaudiArabia 804.14', 1),\n",
       " ('Abadan,Iran Ashdod,Israel 1665.87', 1),\n",
       " ('Abadan,Iran Bagerhat,Bangladesh 5460.74', 1),\n",
       " ('Abadan,Iran Anyang-si,SouthKorea 9478.73', 1),\n",
       " ('Abadan,Iran Amr\\xc4\\x81vati,India 4934.2', 1),\n",
       " ('Abadan,Iran Anantapur,India 5770.16', 1),\n",
       " ('Abadan,Iran Aktobe,Kazakhstan 4574.69', 1),\n",
       " ('Abadan,Iran Amman,Jordan 1499.47', 1),\n",
       " ('Abadan,Iran Ambatt\\xc5\\xabr,India 6059.74', 1),\n",
       " ('Abadan,Iran Allah\\xc4\\x81b\\xc4\\x81d,India 4511.4', 1),\n",
       " ('Abadan,Iran Bandung,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Baicheng,China 9294.5', 1),\n",
       " ('Abadan,Iran Abadan,Iran 0.0', 1),\n",
       " ('Abadan,Iran Anqing,China 8917.77', 1),\n",
       " ('Abadan,Iran Bago,Myanmar 10758.19', 1),\n",
       " ('Abadan,Iran AbuDhabi,UnitedArabEmirates 1433.09', 1),\n",
       " ('Abadan,Iran Al\\xe2\\x80\\x98Am\\xc4\\x81rah,Iraq 236.36', 1),\n",
       " ('Abadan,Iran Aral,China 4592.76', 1),\n",
       " ('Abadan,Iran Ambon,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Amagasaki,Japan 10770.11', 1),\n",
       " ('Abadan,Iran Baguio,Philippines 0.0', 1),\n",
       " ('Abadan,Iran AlHuf\\xc5\\xabf,SaudiArabia 799.84', 1),\n",
       " ('Abadan,Iran Ar\\xc4\\x81k,Iran 651.35', 1),\n",
       " ('Abadan,Iran Al\\xc4\\xabgarh,India 4018.97', 1),\n",
       " ('Abadan,Iran Bahawalpur,Pakistan 3075.06', 1),\n",
       " ('Abadan,Iran Baghdad,Iraq 646.14', 1),\n",
       " ('Abadan,Iran Bal\\xc4\\xb1kesir,Turkey 1807.41', 1),\n",
       " ('Abadan,Iran Aleppo,Syria 1624.59', 1),\n",
       " ('Abadan,Iran AngelesCity,Philippines 0.0', 1),\n",
       " ('Abadan,Iran Antakya,Turkey 1710.28', 1),\n",
       " ('Abadan,Iran Arrah,India 4801.05', 1),\n",
       " ('Abadan,Iran Ahmedabad,India 4643.64', 1),\n",
       " ('Abadan,Iran Akita,Japan 11585.62', 1),\n",
       " ('Abadan,Iran Ad\\xc4\\xb1yaman,Turkey 1584.88', 1),\n",
       " ('Abadan,Iran BandaAceh,Indonesia 0.0', 1),\n",
       " ('Abadan,Iran Aizawl,India 6051.36', 1),\n",
       " ('Abadan,Iran Ashgabat,Turkmenistan 1782.94', 1),\n",
       " ('Abadan,Iran AjmanCity,UnitedArabEmirates 1564.0', 1),\n",
       " ('Abadan,Iran Aomori,Japan 11766.45', 1),\n",
       " ('Abadan,Iran Anshun,China 8703.99', 1),\n",
       " ('Abadan,Iran Ahvaz,Iran 119.84', 1),\n",
       " ('Abadan,Iran Ata\\xc5\\x9fehir,Turkey 1678.7', 1),\n",
       " ('Abadan,Iran Alwar,India 4005.46', 1),\n",
       " ('Abadan,Iran Agartala,India 5749.49', 1),\n",
       " ('Abadan,Iran Antalya,Turkey 1791.42', 1),\n",
       " ('Abadan,Iran ArRayy\\xc4\\x81n,Qatar 1054.89', 1),\n",
       " ('Abadan,Iran Amritsar,India 3435.12', 1),\n",
       " ('Abadan,Iran Adana,Turkey 1807.41', 1),\n",
       " ('Abadan,Iran AsSulaym\\xc4\\x81n\\xc4\\xabyah,Iraq 982.14', 1),\n",
       " ('Abadan,Iran Al\\xe1\\xb8\\xa8udaydah,Yemen 2900.61', 1)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url의 파일을 유니코드 인코딩으로 읽음\n",
    "textFile = sc.textFile(\"input.txt\", use_unicode=True)\n",
    "# utf-8 인코딩을 이용하여 처리\n",
    "counts = textFile.flatMap(lambda line: str(line.encode('utf-8')).split(\"\\n\"))\\\n",
    "          .map(lambda line: (line.split(\"\\t\")[0], 1))\\\n",
    "          .reduceByKey(lambda a, b: a + b)\n",
    "# hdfs에 result 폴더에 저장\n",
    "#counts.saveAsTextFile(\"result.txt\")\n",
    "counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-cba176016315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mformedGraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplitVE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitVE(line):\n",
    "    \"\"\"This function split a row of data into two parts: vertice and edges\n",
    "       The output is a tuple which contains a string of vertice name and a list of it's outward edges\n",
    "    \"\"\"\n",
    "   # line = str(cline).split(' ')\n",
    "    line = line.split(' ')\n",
    "    ver = line[0]\n",
    "    edges = [(line[1], float(line[2][:-2]))]\n",
    "    return (ver, edges)\n",
    "    #return line[2][:-2]\n",
    "data2=\"result.txt\"\n",
    "graph = sc.textFile(data2)\n",
    "formedGraph = graph.map(lambda x: splitVE(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/chris/Downloads/5003/result.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-95327abcf0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mformedGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/chris/Downloads/5003/result.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "formedGraph.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Abadan,Iran', u'Abadan,Iran', u'0.0']\n",
      "[u'Abadan,Iran', u'Abha,SaudiArabia', u'2393.1']\n",
      "[u'Abadan,Iran', u'AbuDhabi,UnitedArabEmirates', u'1433.09']\n",
      "[u'Abadan,Iran', u'AbGhurayb,Iraq', u'622.5']\n",
      "[u'Abadan,Iran', u'AdDnyah,Iraq', u'444.8']\n",
      "[u'Abadan,Iran', u'Adana,Turkey', u'1807.41']\n",
      "[u'Abadan,Iran', u'Aden,Yemen', u'3266.91']\n",
      "[u'Abadan,Iran', u'Adyaman,Turkey', u'1584.88']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"input.txt\")\n",
    "text = file.read().decode(\"utf-8\")\n",
    "file.close()\n",
    "text = text.split('\\n')\n",
    "verticeList = []\n",
    "outverticeList=[]\n",
    "for i in text:\n",
    "    sentence = i.split(' ')\n",
    "    print(sentence)\n",
    "    verticeList.append(str(sentence[0]))\n",
    "    outverticeList.append(str(sentence[0]))\n",
    "    verticeList.append(str(sentence[1]))\n",
    "verticeList = list(verticeList)\n",
    "outverticeList = list(set(outverticeList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
